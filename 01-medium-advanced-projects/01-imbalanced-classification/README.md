# 📁 Module 1: Imbalanced Classification Techniques

## Overview

This module focuses on handling class imbalance in binary classification problems using advanced resampling techniques and cost-sensitive learning strategies.

## Features

- ✅ SMOTE and ADASYN oversampling
- ✅ Tomek links and ENN undersampling
- ✅ Class weight optimization
- ✅ Cost-sensitive learning strategies
- ✅ Evaluation metrics for imbalanced data

## Notebooks

- `01_class_imbalance_analysis.ipynb` - Analyzing and understanding class imbalance
- `02_resampling_techniques.ipynb` - Implementing SMOTE, ADASYN, and undersampling
- `03_cost_sensitive_learning.ipynb` - Cost-sensitive learning and class weights

## Getting Started

These notebooks will guide you through:

1. Detecting and measuring class imbalance
2. Applying various resampling techniques
3. Implementing cost-sensitive learning
4. Evaluating models with appropriate metrics (precision, recall, F1-score, AUC-ROC)

## Requirements

```python
imbalanced-learn>=0.11.0
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
```

## Next Steps

After completing this module, proceed to Module 2: Multi-Class Classification & OvR/OvO Strategies.
